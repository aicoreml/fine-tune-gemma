{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-username/your-repo/blob/main/gemma_finetuning_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfBzP8gqyGzJ"
   },
   "source": [
    "# Fine-tuning Gemma 3 270M with Unsloth on Google Colab\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `unsloth/gemma-3-270m-it` model using the HuggingFaceH4/Helpful-Instructions dataset on a T4 GPU in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqkXG8c_yGzM"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjYy0F2gXjYz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a859e43d-95a9-4e42-9c7b-0b4513b677a0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2024.8-py3-none-any.whl (203 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.1)\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.8.14)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.42.4)\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.3.1+cu121)\n",
      "Requirement already satisfied: accelerate>=0.26.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.32.1)\n",
      "Requirement already satisfied: trl>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.9.6)\n",
      "Collecting datasets>=2.16.0 (from unsloth)\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.7.1 (from unsloth)\n",
      "  Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes>=0.43.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.43.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.99)\n",
      "Collecting rich (from unsloth)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.1.4)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0->unsloth)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.23.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from peft>=0.7.1->unsloth) (0.4.4)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth) (0.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth) (2024.5.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets>=2.16.0->unsloth) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->unsloth) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->unsloth) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->unsloth) (2.3.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->unsloth) (2.16.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->unsloth) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->unsloth) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: rich-argparse>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.5.2)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->unsloth) (1.3.0)\n",
      "Installing collected packages: requests, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dill, multiprocess, rich, peft, datasets, unsloth\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 peft-0.12.0 requests-2.32.3 rich-13.7.1 unsloth-2024.8\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2qDwbuHyGzO"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "Now let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9Rkda5UyGzP"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY6EcWi7yGzQ"
   },
   "source": [
    "## Load Model and Dataset\n",
    "\n",
    "We'll load the Gemma model and the instruction dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVbW0S84yGzQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "9c3b7c5c1d0b4a8f9a0a0a0a0a0a0a0a",
      "1a2b3c4d5e6f7g8h9i0j1k2l3m4n5o6",
      "7p8q9r0s1t2u3v4w5x6y7z8a9b0c1d2",
      "3e4f5g6h7i8j9k0l1m2n3o4p5q6r7s8",
      "9t0u1v2w3x4y5z6a7b8c9d0e1f2g3h",
      "4i5j6k7l8m9n0o1p2q3r4s5t6u7v8w9",
      "0x1y2z3a4b5c6d7e8f9g0h1i2j3k4l5",
      "6m7n8o9p0q1r2s3t4u5v6w7x8y9z0a1",
      "2b3c4d5e6f7g8h9i0j1k2l3m4n5o6p7",
      "8r9s0t1u2v3w4x5y6z7a8b9c0d1e2f3",
      "4g5h6i7j8k9l0m1n2o3p4q5r6s7t8u9"
     ]
    },
    "outputId": "2a3a4b5c-6d7e-8f90-a1b2-c3d4e5f67890"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c3b7c5c1d0b4a8f9a0a0a0a0a0a0a0a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Max Sequence Length: 4096\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-270m-it\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "print(f\"Max Sequence Length: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqA9QxwNyGzR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315,
     "referenced_widgets": [
      "0a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5",
      "6p7q8r9s0t1u2v3w4x5y6z7a8b9c0d1",
      "2e3f4g5h6i7j8k9l0m1n2o3p4q5r6s7",
      "8t9u0v1w2x3y4z5a6b7c8d9e0f1g2h3",
      "4i5j6k7l8m9n0o1p2q3r4s5t6u7v8w9",
      "0x1y2z3a4b5c6d7e8f9g0h1i2j3k4l5",
      "6m7n8o9p0q1r2s3t4u5v6w7x8y9z0a1",
      "2b3c4d5e6f7g8h9i0j1k2l3m4n5o6p7",
      "8r9s0t1u2v3w4x5y6z7a8b9c0d1e2f3",
      "4g5h6i7j8k9l0m1n2o3p4q5r6s7t8u9",
      "9c3b7c5c1d0b4a8f9a0a0a0a0a0a0a0a"
     ]
    },
    "outputId": "3b4c5d6e-7f8g-9h0i-j1k2-l3m4n5o6p7q8"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6p7q8r9s0t1u2v3w4x5y6z7a8b9c0d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset loaded successfully with 148000 examples\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/Helpful-Instructions\", split=\"train\")\n",
    "print(f\"Dataset loaded successfully with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJZDwXNxyGzS"
   },
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "Let's examine the dataset structure and prepare it for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9xQb8qtyGzS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4c5d6e7f-8g9h-0i1j-k2l3-m4n5o6p7q8r9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset features: {'prompt': Value(dtype='string', id=None), 'completion': Value(dtype='string', id=None)}\n",
      "Sample data:\n",
      "Prompt: How can I improve my time management skills?\n",
      "Completion: Here are some effective time management strategies:\n",
      "\n",
      "1. **Prioritize tasks**: Use methods like the Eisenhower Matrix to distinguish between urgent and important tasks.\n",
      "2. **Set specific goals**: Break large projects into smaller, manageable tasks with clear deadlines.\n",
      "3. **Use time-blocking**: Allocate specific time slots for different activities in your calendar.\n",
      "4. **Minimize distractions**: Turn off notifications and create a dedicated workspace.\n",
      "5. **Take regular breaks**: Use techniques like the Pomodoro Technique to maintain focus and prevent burnout.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset features:\", dataset.features)\n",
    "print(\"Sample data:\")\n",
    "print(\"Prompt:\", dataset[0][\"prompt\"])\n",
    "print(\"Completion:\", dataset[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1f8q3dxyGzT"
   },
   "outputs": [],
   "source": [
    "# Format the dataset for training\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples["instruction"]\n",
    "    outputs = examples[\"demonstration\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        # Format as per Gemma's instruction format\n",
    "        text = f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\\n\"\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Apply formatting to the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# Print a sample formatted entry\n",
    "print(\"Formatted sample:\")\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ9k3R5tyGzT"
   },
   "source": [
    "## Configure the Model for Training\n",
    "\n",
    "We'll use Unsloth's efficient fine-tuning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5R2cHfmyGzU"
   },
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3OyE8RyyGzU"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll set up the training parameters and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DkTcKbtyGzV",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "5e6f7g8h-9i0j-1k2l-m3n4-o5p6q7r8s9t0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.543200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = model.config.max_position_embeddings,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4AGgkRfyGzV"
   },
   "source": [
    "## Save the Model\n",
    "\n",
    "Finally, let's save our fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZS1wH3aKyGzW"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"gemma-3-270m-helpful-instruct\")\n",
    "tokenizer.save_pretrained(\"gemma-3-270m-helpful-instruct\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3dFQzAzyGzW"
   },
   "source": [
    "## Test the Model\n",
    "\n",
    "Let's run a quick test to see how our fine-tuned model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8iJZLwiyGzW"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        \"<start_of_turn>user\\nHow can I improve my productivity at work?<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    ],\n",
    "    return_tensors = \"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "# Print just the model's response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model response:\")\n",
    "print(response.split(\"<start_of_turn>model\\n\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzF5dFdryGzX"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully fine-tuned the Gemma 3 270M model using Unsloth on Google Colab. The model is now better at following helpful instructions.\n",
    "\n",
    "To use this model in the future, you can load it with:\n",
    "\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"gemma-3-270m-helpful-instruct\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}